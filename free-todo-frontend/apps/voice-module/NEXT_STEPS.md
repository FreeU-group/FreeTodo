# 音频模块后续计划与思路

## ✅ 已完成

1. **修复保存逻辑**：
   - ✅ 识别结果创建后，立即保存原始转录文本（rawText）
   - ✅ 优化完成后，立即保存优化后的文本（optimizedText）
   - ✅ 停止录音时，保存所有未保存的转录文本

2. **回放一致性保证机制**：
   - ✅ 识别服务记录精确的时间范围
   - ✅ 存储时使用相同的时间范围提取音频
   - ✅ 回放时使用关联的音频文件

---

## 🎯 下一步计划

### 第一阶段：优化实时性和数据流（1-2周）

#### 1.1 降低识别延迟
**目标**：延迟从 2-3秒 降低到 < 1秒

**实施步骤**：
1. **后端优化**：
   ```python
   # 降低处理间隔
   chunk_duration = 0.8  # 从2秒降到0.8秒
   min_samples = 8000    # 最小0.5秒（降低延迟）
   overlap = 0.3         # 减少重叠，降低延迟
   ```

2. **前端优化**：
   - 使用 AudioWorklet 替代 ScriptProcessor（更高效）
   - 每256ms发送一次数据（降低延迟）

3. **测试验证**：
   - 测量实际延迟
   - 确保准确率不下降

#### 1.2 简化数据流
**目标**：单一音频流，避免数据不同步

**实施步骤**：
1. **统一音频源**：
   - 识别和存储都使用同一个 MediaStream
   - 避免时间对齐问题

2. **优化存储逻辑**：
   - 识别结果 → 立即提取音频片段 → 存储
   - 异步处理，不阻塞识别显示

3. **验证机制**：
   - 提取后验证时长是否匹配
   - 记录警告日志

#### 1.3 修复回放一致性
**目标**：确保回放的音频就是识别用的音频

**实施步骤**：
1. **时间戳对齐**：
   - 识别服务记录精确的时间范围（精确到毫秒）
   - 存储时使用相同的时间范围

2. **关联存储**：
   - 音频文件关联识别结果ID
   - 回放时优先使用关联的音频文件

3. **验证机制**：
   - 提取后验证时长
   - 如果差异 > 100ms，记录警告

---

### 第二阶段：集成VAD（2-3周）

#### 2.1 集成语音活动检测
**目标**：只在有语音时识别，降低延迟和资源消耗

**技术选型**：
- **前端**：WebRTC VAD（@ricky0123/vad-web）
- **后端**：Silero VAD（可选）

**实施步骤**：
1. **前端集成**：
   ```typescript
   import { VAD } from '@ricky0123/vad-web';
   
   const vad = new VAD({
     workletURL: '/vad.worklet.bundle.min.js',
     modelURL: '/silero_vad.onnx',
     sampleRate: 16000,
   });
   
   vad.onSpeechStart = () => {
     // 检测到语音开始，立即发送到后端识别
     startRecognition();
   };
   
   vad.onSpeechEnd = () => {
     // 语音结束，获取最终结果
     finalizeRecognition();
   };
   ```

2. **后端集成**（可选）：
   ```python
   from silero_vad import load_silero_vad
   
   vad_model = load_silero_vad()
   
   # 检测语音活动
   speech_prob = vad_model(audio_chunk, sample_rate=16000)
   if speech_prob > 0.5:  # 有语音
       # 立即识别
       result = transcribe(audio_chunk)
   ```

3. **优化识别触发**：
   - 只在检测到语音时触发识别
   - 避免识别静音，降低延迟

#### 2.2 优化识别模型
**目标**：提高实时性和准确率

**技术选型**：
- **FunASR**：专为流式设计，延迟更低（0.5-1秒）
- **Whisper Streaming**：Whisper的流式版本

**实施步骤**：
1. **调研和测试**：
   - 对比 FunASR 和 Faster-Whisper 的性能
   - 测试延迟和准确率

2. **集成新模型**（如果性能更好）：
   - 替换或补充 Faster-Whisper
   - 保持API兼容性

---

### 第三阶段：音色识别（4-6周）

#### 3.1 调研和选型
**目标**：确定音色识别方案

**技术选型**：
- **pyannote.audio**：准确率高，支持实时（需要优化）
- **Resemblyzer**：轻量级，速度快，适合实时
- **SpeechBrain**：准确率最高，但资源消耗大

**调研内容**：
1. 各方案的准确率对比
2. 实时性能测试
3. 资源消耗评估
4. 集成难度评估

#### 3.2 实现说话人识别
**目标**：区分不同说话人

**实施步骤**：
1. **集成选定的方案**：
   ```python
   # 示例：使用 pyannote.audio
   from pyannote.audio import Pipeline
   
   pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization")
   
   # 处理音频
   diarization = pipeline(audio_file)
   
   # 获取说话人信息
   for turn, _, speaker in diarization.itertracks(yield_label=True):
       print(f"说话人 {speaker}: {turn.start:.2f}s - {turn.end:.2f}s")
   ```

2. **实时处理优化**：
   - 优化模型加载时间
   - 批量处理，提高效率
   - 缓存说话人特征

3. **UI显示说话人标签**：
   - 在转录结果中显示说话人
   - 支持说话人筛选和搜索

#### 3.3 说话人管理
**目标**：支持说话人注册和管理

**功能**：
1. **说话人注册**：
   - 录制说话人样本
   - 提取说话人特征
   - 保存说话人信息

2. **说话人识别**：
   - 实时识别说话人
   - 显示说话人标签

3. **说话人管理**：
   - 编辑说话人信息
   - 删除说话人
   - 合并说话人

---

## 💡 技术思路

### 1. 实时性优化思路

**问题**：当前延迟2-3秒，用户体验不够实时

**解决思路**：
1. **降低处理间隔**：从2秒降到0.8秒
2. **使用VAD**：只在有语音时识别
3. **优化模型**：使用更快的识别模型（FunASR）
4. **前端优化**：使用 AudioWorklet 替代 ScriptProcessor

### 2. 数据流优化思路

**问题**：识别和存储使用不同数据源，可能不同步

**解决思路**：
1. **统一音频源**：识别和存储都使用同一个 MediaStream
2. **精确时间戳**：记录处理的时间范围（精确到毫秒）
3. **时间对齐**：存储时使用相同的时间范围提取音频
4. **验证机制**：提取后验证时长是否匹配

### 3. 音色识别思路

**问题**：需要区分不同说话人

**解决思路**：
1. **分阶段实施**：
   - 先做基础功能（实时转录）
   - 再考虑音色识别
   - 先支持2-3个说话人，再扩展

2. **技术选型**：
   - 优先考虑实时性能
   - 平衡准确率和资源消耗
   - 选择易于集成的方案

3. **实现策略**：
   - 先做离线处理（录音后处理）
   - 再优化为实时处理
   - 逐步提高准确率

---

## 📊 优先级排序

### 高优先级（立即实施）
1. ✅ **修复保存逻辑**：确保原始和优化后的文本都能保存
2. ✅ **修复回放一致性**：确保回放的音频就是识别用的音频
3. **降低识别延迟**：从2-3秒降到<1秒
4. **简化数据流**：统一音频源，避免不同步

### 中优先级（1-2个月）
1. **集成VAD**：只在有语音时识别
2. **优化识别模型**：考虑 FunASR 或 Whisper Streaming
3. **前端优化**：使用 AudioWorklet 替代 ScriptProcessor

### 低优先级（3-6个月）
1. **音色识别**：区分不同说话人
2. **说话人管理**：注册、编辑、删除说话人
3. **高级功能**：关键词高亮、错误标记等

---

## 🔧 技术债务

### 需要重构的部分
1. **数据流**：简化识别和存储的数据流
2. **错误处理**：统一错误处理机制
3. **状态管理**：优化 Zustand store 结构
4. **代码组织**：拆分大文件，提高可维护性

### 需要优化的部分
1. **性能**：优化音频处理性能
2. **内存**：优化内存使用
3. **网络**：优化网络请求
4. **用户体验**：优化UI交互

---

## 📝 注意事项

1. **向后兼容**：确保新功能不影响现有功能
2. **测试覆盖**：增加单元测试和集成测试
3. **文档更新**：及时更新技术文档
4. **性能监控**：监控系统性能，及时发现问题

---

## 🎯 成功标准

### 第一阶段成功标准
- ✅ 原始和优化后的文本都能保存
- ✅ 回放的音频就是识别用的音频
- ⏳ 识别延迟 < 1秒
- ⏳ 数据流简化，无同步问题

### 第二阶段成功标准
- ⏳ VAD集成，只在有语音时识别
- ⏳ 识别延迟 < 0.5秒
- ⏳ 资源消耗降低30%

### 第三阶段成功标准
- ⏳ 音色识别准确率 > 80%
- ⏳ 支持2-3个说话人
- ⏳ 实时识别说话人

---

## 📚 参考资料

1. **实时语音识别**：
   - FunASR: https://github.com/alibaba-damo-academy/FunASR
   - Whisper Streaming: https://github.com/ggerganov/whisper.cpp

2. **语音活动检测**：
   - WebRTC VAD: https://github.com/ricky0123/vad-web
   - Silero VAD: https://github.com/snakers4/silero-vad

3. **音色识别**：
   - pyannote.audio: https://github.com/pyannote/pyannote-audio
   - Resemblyzer: https://github.com/resemble-ai/Resemblyzer
   - SpeechBrain: https://github.com/speechbrain/speechbrain

