"use client";

import { useEffect, useRef, useState } from 'react';
import WaveformTimeline from './components/WaveformTimeline';
import TranscriptionLog from './components/TranscriptionLog';
import ChatInterface from './components/ChatInterface';
import ScheduleList from './components/ScheduleList';
import { useAppStore } from './store/useAppStore';
import { RecordingService } from './services/RecordingService';
import { RecognitionService } from './services/RecognitionService';
import { WebSocketRecognitionService } from './services/WebSocketRecognitionService';
import { OptimizationService } from './services/OptimizationService';
import { ScheduleExtractionService } from './services/ScheduleExtractionService';
import { PersistenceService } from './services/PersistenceService';
import { TranscriptSegment, ChatMessage, AudioSegment, ScheduleItem } from './types';

const SYSTEM_PROMPT_CHAT = `
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æœ€è¿‘10åˆ†é’Ÿçš„è¯­éŸ³è½¬å½•ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
å¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ã€‚
`;

export function VoiceModulePanel() {
  const {
    isRecording,
    recordingStartTime,
    currentTime,
    timeline,
    transcripts,
    schedules,
    audioSegments,
    processStatus,
    startRecording: storeStartRecording,
    stopRecording: storeStopRecording,
    setCurrentTime: storeSetCurrentTime,
    setTimelineView,
    setTimelineZoom,
    addTranscript,
    updateTranscript,
    addSchedule,
    addAudioSegment,
    updateAudioSegment,
    setProcessStatus,
  } = useAppStore();

  const recordingServiceRef = useRef<RecordingService | null>(null);
  const recognitionServiceRef = useRef<RecognitionService | null>(null);
  const websocketRecognitionServiceRef = useRef<WebSocketRecognitionService | null>(null);
  const optimizationServiceRef = useRef<OptimizationService | null>(null);
  const scheduleExtractionServiceRef = useRef<ScheduleExtractionService | null>(null);
  const persistenceServiceRef = useRef<PersistenceService | null>(null);

  const [chatMessages, setChatMessages] = useState<ChatMessage[]>([
    { 
      id: 'init', 
      role: 'model', 
      text: 'ä½ å¥½ï¼æˆ‘æ˜¯åŸºäº DeepSeek çš„ 7Ã—24 æ™ºèƒ½å½•éŸ³åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥æŒç»­å½•éŸ³ã€è¯†åˆ«è¯­éŸ³å¹¶è‡ªåŠ¨æå–æ—¥ç¨‹ã€‚ä½ å¯ä»¥éšæ—¶å‘æˆ‘æé—®ï¼Œå¦‚æœæœ‰å½•éŸ³å†…å®¹ï¼Œæˆ‘ä¼šåŸºäºæœ€è¿‘çš„å½•éŸ³å†…å®¹å›ç­”ï¼›å¦‚æœæ²¡æœ‰å½•éŸ³å†…å®¹ï¼Œæˆ‘ä¹Ÿå¯ä»¥å›ç­”ä¸€èˆ¬æ€§é—®é¢˜ã€‚', 
      timestamp: new Date() 
    }
  ]);
  const [isChatLoading, setIsChatLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [isApiKeyMissing, setIsApiKeyMissing] = useState(false);
  const [analyser, setAnalyser] = useState<AnalyserNode | null>(null);
  const [isPlaying, setIsPlaying] = useState(false);
  const [audioSource, setAudioSource] = useState<'microphone' | 'system'>('microphone');

  const audioPlayerRef = useRef<HTMLAudioElement | null>(null);
  const playbackIntervalRef = useRef<number | null>(null);

  // åˆå§‹åŒ–æœåŠ¡
  useEffect(() => {
    const recordingService = new RecordingService(audioSource);
    recordingService.setCallbacks({
      onSegmentReady: handleAudioSegmentReady,
      onError: (err) => {
        console.error('Recording error:', err);
        setError(err.message);
        setProcessStatus('recording', 'error');
      },
      onAudioData: (analyserNode) => {
        setAnalyser(analyserNode);
      },
    });
    recordingServiceRef.current = recordingService;

    // Web Speech API è¯†åˆ«æœåŠ¡ï¼ˆç”¨äºéº¦å…‹é£ï¼‰
    const recognitionService = new RecognitionService();
    recognitionService.setCallbacks({
      onResult: handleRecognitionResult,
      onError: (err) => {
        console.error('Recognition error:', err);
        setError(err.message);
        setProcessStatus('recognition', 'error');
      },
      onStatusChange: (status) => {
        setProcessStatus('recognition', status);
      },
    });
    recognitionServiceRef.current = recognitionService;
    
    // WebSocket Faster-Whisper è¯†åˆ«æœåŠ¡ï¼ˆç”¨äºç³»ç»ŸéŸ³é¢‘å’Œé«˜è´¨é‡è¯†åˆ«ï¼‰
    const websocketRecognitionService = new WebSocketRecognitionService();
    websocketRecognitionService.setCallbacks({
      onResult: handleRecognitionResult,
      onError: (err) => {
        console.error('WebSocket recognition error:', err);
        setError(err.message);
        setProcessStatus('recognition', 'error');
      },
      onStatusChange: (status) => {
        setProcessStatus('recognition', status);
      },
    });
    websocketRecognitionServiceRef.current = websocketRecognitionService;

    const optimizationService = new OptimizationService();
    optimizationService.setCallbacks({
      onOptimized: handleTextOptimized,
      onError: (segmentId, err) => {
        console.error(`Optimization error for ${segmentId}:`, err);
        setProcessStatus('optimization', 'error');
      },
      onStatusChange: (status) => {
        setProcessStatus('optimization', status);
      },
    });
    optimizationServiceRef.current = optimizationService;

    const scheduleExtractionService = new ScheduleExtractionService();
    scheduleExtractionService.setCallbacks({
      onScheduleExtracted: handleScheduleExtracted,
      onError: (err) => {
        console.error('Schedule extraction error:', err);
        setProcessStatus('scheduleExtraction', 'error');
      },
      onStatusChange: (status) => {
        setProcessStatus('scheduleExtraction', status);
      },
    });
    scheduleExtractionServiceRef.current = scheduleExtractionService;

    const persistenceService = new PersistenceService();
    persistenceService.setCallbacks({
      onUploadProgress: (type, progress) => {
        console.log(`Upload progress (${type}): ${progress}%`);
      },
      onError: (err) => {
        console.error('Persistence error:', err);
        setProcessStatus('persistence', 'error');
      },
      onStatusChange: (status) => {
        setProcessStatus('persistence', status);
      },
    });
    persistenceServiceRef.current = persistenceService;

    const audio = new Audio();
    audioPlayerRef.current = audio;
    audio.onended = () => {
      setIsPlaying(false);
      if (playbackIntervalRef.current) clearInterval(playbackIntervalRef.current);
    };
    audio.onpause = () => {
      setIsPlaying(false);
      if (playbackIntervalRef.current) clearInterval(playbackIntervalRef.current);
    };
    audio.onplay = () => {
      setIsPlaying(true);
      if (playbackIntervalRef.current) clearInterval(playbackIntervalRef.current);
      playbackIntervalRef.current = window.setInterval(() => {
        if (audio.currentTime) {
          storeSetCurrentTime(new Date(Date.now() - (audio.duration - audio.currentTime) * 1000));
        }
      }, 100);
    };

    let apiKey = process.env.NEXT_PUBLIC_DEEPSEEK_API_KEY;
    if (!apiKey || apiKey.includes('your_deepseek_api_key')) {
      apiKey = "sk-26d76c61cf2842fcb729e019d587a026";
    }
    setIsApiKeyMissing(!apiKey);

    return () => {
      recordingService.stop();
      recognitionService.stop();
      websocketRecognitionService.stop();
      if (playbackIntervalRef.current) clearInterval(playbackIntervalRef.current);
      audio.pause();
    };
  }, [audioSource]);

  useEffect(() => {
    const interval = setInterval(() => {
      storeSetCurrentTime(new Date());
    }, 1000);
    return () => clearInterval(interval);
  }, []);

  const handleAudioSegmentReady = async (blob: Blob, startTime: Date, endTime: Date, segmentId: string, source: 'microphone' | 'system') => {
    const audioSegment: AudioSegment = {
      id: segmentId,
      startTime,
      endTime,
      duration: endTime.getTime() - startTime.getTime(),
      fileSize: blob.size,
      audioSource: source,
      uploadStatus: 'pending',
    };
    addAudioSegment(audioSegment);

    if (persistenceServiceRef.current) {
      updateAudioSegment(segmentId, { uploadStatus: 'uploading' });
      const audioFileId = await persistenceServiceRef.current.uploadAudio(blob, {
        startTime,
        endTime,
        segmentId,
      });
      if (audioFileId) {
        // è·å–éŸ³é¢‘æ–‡ä»¶ URL
        const audioUrl = await persistenceServiceRef.current.getAudioUrl(audioFileId);
        updateAudioSegment(segmentId, { 
          fileUrl: audioUrl || undefined, 
          uploadStatus: 'uploaded' 
        });
      } else {
        updateAudioSegment(segmentId, { uploadStatus: 'failed' });
      }
    }
  };

  const handleRecognitionResult = (text: string, isFinal: boolean) => {
    if (!text.trim()) return;
    const currentRecordingStartTime = useAppStore.getState().recordingStartTime;
    if (!currentRecordingStartTime) return;

    const now = Date.now();
    const relativeEndTime = now - currentRecordingStartTime.getTime();
    const relativeStartTime = Math.max(0, relativeEndTime - 2000);
    const absoluteEnd = new Date();
    const absoluteStart = new Date(absoluteEnd.getTime() - Math.max(500, relativeEndTime - relativeStartTime));

    const lastSegment = useAppStore.getState().audioSegments[useAppStore.getState().audioSegments.length - 1];
    const transcripts = useAppStore.getState().transcripts;
    
    if (isFinal) {
      // æœ€ç»ˆç»“æœï¼šæ€»æ˜¯åˆ›å»ºæ–°ç‰‡æ®µï¼ˆä¿ç•™å†å²è®°å½•ï¼‰
      // æ£€æŸ¥æ˜¯å¦ä¸æœ€åä¸€ä¸ªæœ€ç»ˆç‰‡æ®µå†…å®¹ç›¸åŒï¼ˆé¿å…é‡å¤ï¼‰
      const lastFinalSegment = [...transcripts].reverse().find(t => !t.isInterim);
      if (lastFinalSegment && lastFinalSegment.rawText === text) {
        // å†…å®¹ç›¸åŒï¼Œå¯èƒ½æ˜¯é‡å¤å‘é€ï¼Œè·³è¿‡
        console.log('è·³è¿‡é‡å¤çš„è¯†åˆ«ç»“æœ:', text);
        return;
      }
      
      // å¦‚æœæœ‰ä¸´æ—¶ç‰‡æ®µï¼Œå…ˆå°†å…¶è½¬ä¸ºæœ€ç»ˆç»“æœ
      const lastInterimSegment = [...transcripts].reverse().find(t => t.isInterim);
      if (lastInterimSegment && lastInterimSegment.interimText === text) {
        // ä¸´æ—¶ç‰‡æ®µå†…å®¹ä¸æœ€ç»ˆç»“æœç›¸åŒï¼Œç›´æ¥è½¬ä¸ºæœ€ç»ˆç»“æœ
        updateTranscript(lastInterimSegment.id, {
          rawText: text,
          isInterim: false,
          interimText: undefined,
          absoluteStart,
          absoluteEnd,
          audioStart: relativeStartTime,
          audioEnd: relativeEndTime,
        });
        
        // è§¦å‘ä¼˜åŒ–
        const updatedSegment = { ...lastInterimSegment, rawText: text, isInterim: false };
        if (optimizationServiceRef.current) {
          optimizationServiceRef.current.enqueue(updatedSegment);
        }
      } else {
        // åˆ›å»ºæ–°çš„æœ€ç»ˆç‰‡æ®µ
        const segment: TranscriptSegment = {
          id: `transcript_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`,
          timestamp: new Date(),
          absoluteStart,
          absoluteEnd,
          segmentId: lastSegment?.id,
          rawText: text,
          isOptimized: false,
          isInterim: false,
          containsSchedule: false,
          audioStart: relativeStartTime,
          audioEnd: relativeEndTime,
          uploadStatus: 'pending',
        };
        addTranscript(segment);
        if (optimizationServiceRef.current) {
          optimizationServiceRef.current.enqueue(segment);
        }
      }
    } else {
      // ä¸´æ—¶ç»“æœï¼šæ›´æ–°æœ€åä¸€ä¸ªä¸´æ—¶ç‰‡æ®µæˆ–åˆ›å»ºæ–°çš„ä¸´æ—¶ç‰‡æ®µ
      const lastInterimSegment = [...transcripts].reverse().find(t => t.isInterim);
      
      if (lastInterimSegment) {
        // æ›´æ–°ä¸´æ—¶æ–‡æœ¬
        updateTranscript(lastInterimSegment.id, {
          interimText: text,
          absoluteEnd,
          audioEnd: relativeEndTime,
        });
      } else {
        // åˆ›å»ºæ–°çš„ä¸´æ—¶ç‰‡æ®µ
        const segment: TranscriptSegment = {
          id: `transcript_interim_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`,
          timestamp: new Date(),
          absoluteStart,
          absoluteEnd,
          segmentId: lastSegment?.id,
          rawText: '', // ä¸´æ—¶ç»“æœæ—¶ä¸ºç©º
          interimText: text,
          isOptimized: false,
          isInterim: true,
          containsSchedule: false,
          audioStart: relativeStartTime,
          audioEnd: relativeEndTime,
          uploadStatus: 'pending',
        };
        addTranscript(segment);
      }
    }
  };

  const handleTextOptimized = (segmentId: string, optimizedText: string, containsSchedule: boolean) => {
    updateTranscript(segmentId, {
      optimizedText,
      isOptimized: true,
      containsSchedule,
    });

    // æ— è®º containsSchedule æ˜¯å¦ä¸º trueï¼Œéƒ½å°è¯•æå–æ—¥ç¨‹
    // å› ä¸ºå³ä½¿ LLM æ²¡æœ‰æ ‡è®°ï¼Œæ–‡æœ¬ä¸­ä¹Ÿå¯èƒ½åŒ…å«æ—¶é—´ä¿¡æ¯
    if (scheduleExtractionServiceRef.current) {
      // ä»æœ€æ–°çš„çŠ¶æ€ä¸­è·å– segmentï¼ˆå› ä¸º updateTranscript å¯èƒ½è¿˜æ²¡æ›´æ–° transcriptsï¼‰
      const currentTranscripts = useAppStore.getState().transcripts;
      const segment = currentTranscripts.find(t => t.id === segmentId);
      if (segment) {
        console.log(`[VoiceModule] å°†ç‰‡æ®µ ${segmentId} åŠ å…¥æ—¥ç¨‹æå–é˜Ÿåˆ—ï¼Œæ–‡æœ¬: ${optimizedText.substring(0, 50)}...`);
        scheduleExtractionServiceRef.current.enqueue({
          ...segment,
          optimizedText,
          isOptimized: true,
          containsSchedule,
        });
      } else {
        console.warn(`[VoiceModule] æœªæ‰¾åˆ°ç‰‡æ®µ ${segmentId}ï¼Œæ— æ³•æå–æ—¥ç¨‹`);
      }
    } else {
      console.warn(`[VoiceModule] scheduleExtractionServiceRef.current ä¸ºç©ºï¼Œæ— æ³•æå–æ—¥ç¨‹`);
    }

    setTimeout(() => {
      const currentTranscripts = useAppStore.getState().transcripts;
      const pendingTranscripts = currentTranscripts.filter(t => t.isOptimized && t.uploadStatus === 'pending');
      if (pendingTranscripts.length >= 10 && persistenceServiceRef.current) {
        persistenceServiceRef.current.saveTranscripts(pendingTranscripts);
        pendingTranscripts.forEach(t => {
          updateTranscript(t.id, { uploadStatus: 'uploaded' });
        });
      }
    }, 100);
  };

  const handleScheduleExtracted = (schedule: ScheduleItem) => {
    console.log(`[VoiceModule] æ”¶åˆ°æå–çš„æ—¥ç¨‹: ${schedule.description} (æ—¶é—´: ${schedule.scheduleTime.toLocaleString()})`);
    addSchedule(schedule);
    console.log(`[VoiceModule] æ—¥ç¨‹å·²æ·»åŠ åˆ°çŠ¶æ€ï¼Œå½“å‰æ—¥ç¨‹æ•°: ${useAppStore.getState().schedules.length}`);
    setTimeout(() => {
      const currentSchedules = useAppStore.getState().schedules;
      const pendingSchedules = currentSchedules.filter(s => s.status === 'pending');
      if (pendingSchedules.length >= 5 && persistenceServiceRef.current) {
        persistenceServiceRef.current.saveSchedules(pendingSchedules);
      }
    }, 100);
  };

  const handleStartRecording = async () => {
    setError(null);
    storeStartRecording();
    try {
      if (recordingServiceRef.current) {
        // æ›´æ–°éŸ³é¢‘æº
        recordingServiceRef.current.setAudioSource(audioSource);
        
        // å¦‚æœæ˜¯ç³»ç»ŸéŸ³é¢‘ï¼Œæç¤ºç”¨æˆ·
        if (audioSource === 'system') {
          // æµè§ˆå™¨ä¼šè‡ªåŠ¨å¼¹å‡ºé€‰æ‹©çª—å£ï¼Œè¿™é‡Œå¯ä»¥æ·»åŠ æç¤º
          console.log('è¯·åœ¨å¼¹å‡ºçš„çª—å£ä¸­é€‰æ‹©è¦å…±äº«çš„æ ‡ç­¾é¡µï¼ˆåŒ…å«éŸ³é¢‘ï¼‰');
        }
        
        await recordingServiceRef.current.start();
        setProcessStatus('recording', 'running');
      }
      // æ ¹æ®éŸ³é¢‘æºé€‰æ‹©è¯†åˆ«æœåŠ¡
      if (audioSource === 'microphone') {
        // éº¦å…‹é£ï¼šä½¿ç”¨ Web Speech APIï¼ˆå¿«é€Ÿã€å…è´¹ï¼‰
        if (recognitionServiceRef.current) {
          setTimeout(() => {
            recognitionServiceRef.current?.start();
          }, 500);
        }
      } else if (audioSource === 'system') {
        // ç³»ç»ŸéŸ³é¢‘ï¼šä½¿ç”¨ WebSocket Faster-Whisperï¼ˆæ”¯æŒç³»ç»ŸéŸ³é¢‘ï¼Œæ›´å‡†ç¡®ï¼‰
        // ç­‰å¾…å½•éŸ³æœåŠ¡å®Œå…¨å¯åŠ¨åå†è·å–æµ
        setTimeout(() => {
          if (recordingServiceRef.current && websocketRecognitionServiceRef.current) {
            const stream = recordingServiceRef.current.getStream();
            if (stream) {
              console.log('å¯åŠ¨ WebSocket Faster-Whisper è¯†åˆ«...');
              websocketRecognitionServiceRef.current.start(stream);
            } else {
              console.warn('æ— æ³•è·å–éŸ³é¢‘æµï¼ŒWebSocket è¯†åˆ«æœªå¯åŠ¨');
              setError('æ— æ³•è·å–éŸ³é¢‘æµï¼Œè¯·é‡è¯•');
            }
          }
        }, 1000); // ç­‰å¾…å½•éŸ³æœåŠ¡å®Œå…¨å¯åŠ¨
      }
    } catch (err) {
      const error = err instanceof Error ? err : new Error('Failed to start recording');
      console.error('Recording error:', error);
      setError(error.message);
      setProcessStatus('recording', 'error');
      storeStopRecording();
    }
  };

  const handleStopRecording = async () => {
    if (recordingServiceRef.current) {
      await recordingServiceRef.current.stop();
      setProcessStatus('recording', 'idle');
    }
    if (recognitionServiceRef.current) {
      recognitionServiceRef.current.stop();
    }
    if (websocketRecognitionServiceRef.current) {
      websocketRecognitionServiceRef.current.stop();
    }
    storeStopRecording();
  };

  const handleSeek = async (time: Date) => {
    if (isRecording) return;
    const segment = audioSegments.find(s => s.startTime <= time && s.endTime >= time);
    if (segment && audioPlayerRef.current) {
      // å¦‚æœè¿˜æ²¡æœ‰ fileUrlï¼Œå°è¯•è·å–
      let audioUrl = segment.fileUrl;
      if (!audioUrl && segment.id && persistenceServiceRef.current) {
        const fetchedUrl = await persistenceServiceRef.current.getAudioUrl(segment.id);
        if (fetchedUrl) {
          audioUrl = fetchedUrl;
          updateAudioSegment(segment.id, { fileUrl: fetchedUrl });
        }
      }
      
      if (audioUrl) {
        audioPlayerRef.current.src = audioUrl;
        const seekTime = (time.getTime() - segment.startTime.getTime()) / 1000;
        audioPlayerRef.current.currentTime = seekTime;
        audioPlayerRef.current.play();
      }
    }
  };

  const handleTimelineChange = (startTime: Date, duration: number) => {
    setTimelineView(startTime, duration);
    const endTime = new Date(startTime.getTime() + duration);
    if (persistenceServiceRef.current) {
      persistenceServiceRef.current.queryTranscripts(startTime, endTime).then(fetched => {
        const existing = useAppStore.getState().transcripts;
        fetched.forEach(segment => {
          if (!existing.find(t => t.id === segment.id)) {
            addTranscript(segment);
          }
        });
      });
      persistenceServiceRef.current.querySchedules(startTime, endTime).then(fetched => {
        const existing = useAppStore.getState().schedules;
        fetched.forEach(schedule => {
          if (!existing.find(s => s.id === schedule.id)) {
            addSchedule(schedule);
          }
        });
      });
    }
  };

  const handleSegmentClick = async (
    startMs: number,
    endMs: number,
    segmentId?: string,
    absoluteStartMs?: number
  ) => {
    if (isRecording || !recordingStartTime) return;
    let targetSegment = segmentId ? audioSegments.find(s => s.id === segmentId) : undefined;
    if (!targetSegment && absoluteStartMs) {
      const abs = new Date(absoluteStartMs);
      targetSegment = audioSegments.find(s => s.startTime <= abs && s.endTime >= abs);
    }
    if (!targetSegment) {
      const startTime = new Date(recordingStartTime.getTime() + startMs);
      await handleSeek(startTime);
      return;
    }
    if (audioPlayerRef.current) {
      // å¦‚æœè¿˜æ²¡æœ‰ fileUrlï¼Œå°è¯•è·å–
      let audioUrl = targetSegment.fileUrl;
      if (!audioUrl && targetSegment.id && persistenceServiceRef.current) {
        const fetchedUrl = await persistenceServiceRef.current.getAudioUrl(targetSegment.id);
        if (fetchedUrl) {
          audioUrl = fetchedUrl;
          updateAudioSegment(targetSegment.id, { fileUrl: fetchedUrl });
        }
      }
      
      if (audioUrl) {
        audioPlayerRef.current.src = audioUrl;
        let seekSeconds = 0;
        if (absoluteStartMs) {
          seekSeconds = Math.max(0, (absoluteStartMs - targetSegment.startTime.getTime()) / 1000);
        } else {
          seekSeconds = Math.max(0, (startMs - (targetSegment.startTime.getTime() - recordingStartTime.getTime())) / 1000);
        }
        audioPlayerRef.current.currentTime = seekSeconds;
        audioPlayerRef.current.play();
        return;
      }
    }
    const startTime = new Date(recordingStartTime.getTime() + startMs);
    await handleSeek(startTime);
  };

  const handleSendMessage = async (text: string) => {
    if (!text.trim()) return;
    const msgId = Date.now().toString();
    setChatMessages(prev => [...prev, { id: msgId, role: 'user', text, timestamp: new Date() }]);
    setIsChatLoading(true);
    
    // åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯å ä½ç¬¦
    const assistantMsgId = (Date.now() + 1).toString();
    setChatMessages(prev => [...prev, {
      id: assistantMsgId,
      role: 'model',
      text: '',
      timestamp: new Date()
    }]);
    
    try {
      // è·å–æœ€è¿‘ 10 åˆ†é’Ÿçš„è½¬å½•å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
      const tenMinutesAgo = new Date(Date.now() - 10 * 60 * 1000);
      const contextSegments = transcripts.filter(t => t.timestamp > tenMinutesAgo);
      const contextString = contextSegments
        .map(t => `[${t.timestamp.toLocaleTimeString()}] ${t.optimizedText || t.rawText}`)
        .join('\n');
      
      // ä½¿ç”¨ Next.js ä»£ç†è·¯å¾„ï¼ˆä¼šè‡ªåŠ¨ä»£ç†åˆ°åç«¯ localhost:8000ï¼‰
      let apiUrl: string;
      if (typeof window !== 'undefined' && window.location) {
        const origin = window.location.origin;
        if (!origin || origin === 'null' || origin === 'undefined') {
          apiUrl = 'http://localhost:3000/api/deepseek/chat/completions';
        } else {
          apiUrl = `${origin}/api/deepseek/chat/completions`;
        }
      } else {
        apiUrl = 'http://localhost:8000/api/deepseek/chat/completions';
      }
      
      // æ„å»ºæ¶ˆæ¯ï¼šå¦‚æœæœ‰ä¸Šä¸‹æ–‡å°±åŠ ä¸Šï¼Œæ²¡æœ‰å°±åªå‘é€ç”¨æˆ·é—®é¢˜
      const systemPrompt = contextString 
        ? SYSTEM_PROMPT_CHAT 
        : 'ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€‚ä½ å¯ä»¥å›ç­”ç”¨æˆ·çš„å„ç§é—®é¢˜ã€‚å¦‚æœç”¨æˆ·è¯¢é—®å…³äºå½•éŸ³å†…å®¹çš„é—®é¢˜ï¼Œè¯·å‘ŠçŸ¥ç”¨æˆ·ç›®å‰æ²¡æœ‰å½•éŸ³å†…å®¹ï¼Œå»ºè®®å…ˆå¼€å§‹å½•éŸ³ã€‚';
      
      const userContent = contextString
        ? `ä¸Šä¸‹æ–‡ (æœ€è¿‘10åˆ†é’Ÿ):\n${contextString}\n\nç”¨æˆ·æé—®: ${text}`
        : text;
      
      // ä½¿ç”¨æµå¼ API
      const response = await fetch(apiUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'deepseek-chat',
          messages: [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: userContent }
          ],
          temperature: 0.7,
          stream: true,
        }),
      });
      
      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }
      
      // è¯»å–æµå¼å“åº”
      const reader = response.body?.getReader();
      const decoder = new TextDecoder();
      let buffer = '';
      let fullText = '';
      
      if (!reader) {
        throw new Error('æ— æ³•è¯»å–å“åº”æµ');
      }
      
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        
        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || ''; // ä¿ç•™æœ€åä¸€ä¸ªä¸å®Œæ•´çš„è¡Œ
        
        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            if (data === '[DONE]') {
              continue;
            }
            
            try {
              const chunk = JSON.parse(data);
              if (chunk.choices && chunk.choices[0]?.delta?.content) {
                const content = chunk.choices[0].delta.content;
                fullText += content;
                
                // å®æ—¶æ›´æ–°æ¶ˆæ¯å†…å®¹
                setChatMessages(prev => prev.map(msg => 
                  msg.id === assistantMsgId 
                    ? { ...msg, text: fullText }
                    : msg
                ));
              }
            } catch (e) {
              console.error('è§£ææµå¼æ•°æ®å¤±è´¥:', e, data);
            }
          }
        }
      }
      
    } catch (e: any) {
      console.error('Chat API error:', e);
      const errorMessage = e?.message || e?.toString() || "Unknown error";
      setChatMessages(prev => prev.map(msg => 
        msg.id === assistantMsgId 
          ? { ...msg, text: `å‡ºé”™: ${errorMessage}ã€‚è¯·æ£€æŸ¥åç«¯æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œï¼Œä»¥åŠ LLM é…ç½®æ˜¯å¦æ­£ç¡®ã€‚` }
          : msg
      ));
    } finally {
      setIsChatLoading(false);
    }
  };

  return (
    <div className="flex h-full flex-col overflow-hidden bg-background text-foreground">
      <header className="shrink-0 border-b border-border bg-card/80 backdrop-blur flex items-center justify-between px-6 py-3 relative">
        <div className="flex items-center gap-2">
          <div className={`w-3 h-3 rounded-full ${isRecording ? 'bg-red-500 animate-pulse' : 'bg-muted'}`}></div>
          <h1 className="font-bold text-lg tracking-tight text-foreground">
            7Ã—24 æ™ºèƒ½å½•éŸ³åŠ©æ‰‹
          </h1>
        </div>
        {error && (
          <div className={`absolute left-1/2 top-1/2 -translate-x-1/2 -translate-y-1/2 px-4 py-1 rounded-full text-xs font-medium ${
            error.includes('ç³»ç»ŸéŸ³é¢‘æ¨¡å¼') 
              ? 'bg-amber-500/10 text-amber-600 border border-amber-500/20' 
              : 'bg-red-500/10 text-red-600 border border-red-500/20 animate-pulse'
          }`}>
            {error}
          </div>
        )}
        <div className="flex items-center gap-3">
          <div className="flex items-center gap-2 text-xs text-muted-foreground">
            <span className={`w-2 h-2 rounded-full ${processStatus.recording === 'running' ? 'bg-green-500' : 'bg-muted'}`}></span>
            <span>å½•éŸ³</span>
            <span className={`w-2 h-2 rounded-full ${processStatus.recognition === 'running' ? 'bg-green-500' : 'bg-muted'}`}></span>
            <span>è¯†åˆ«</span>
          </div>
          {!isRecording && (
            <select
              value={audioSource}
              onChange={(e) => setAudioSource(e.target.value as 'microphone' | 'system')}
              className="bg-background border border-input text-foreground text-xs px-2 py-1 rounded focus:outline-none focus:ring-2 focus:ring-primary/50"
              title="é€‰æ‹©éŸ³é¢‘æ¥æº"
            >
              <option value="microphone">ğŸ¤ éº¦å…‹é£ï¼ˆWeb Speech APIï¼‰</option>
              <option value="system">ğŸ”Š ç³»ç»ŸéŸ³é¢‘ï¼ˆFaster-Whisper å®æ—¶è¯†åˆ«ï¼‰</option>
            </select>
          )}
          {!isRecording ? (
            <button
              onClick={handleStartRecording}
              className="bg-primary hover:bg-primary/90 text-primary-foreground px-4 py-1.5 rounded-lg text-sm font-medium transition-all shadow-sm flex items-center gap-2"
            >
              å¼€å§‹å½•éŸ³
            </button>
          ) : (
            <button
              onClick={handleStopRecording}
              className="bg-red-500/10 hover:bg-red-500/20 text-red-600 border border-red-500/50 px-4 py-1.5 rounded-lg text-sm font-medium transition-all flex items-center gap-2"
            >
              åœæ­¢å½•éŸ³
            </button>
          )}
        </div>
      </header>
      <main className="flex-1 grid grid-cols-1 md:grid-cols-3 overflow-hidden">
        <div className="md:col-span-2 flex flex-col h-full overflow-hidden border-r border-border">
          <div className="h-72 p-4 shrink-0 bg-card/50 flex flex-col">
            <div className="text-sm text-foreground mb-3 font-semibold flex justify-between items-center">
              <span>æ—¶é—´è½´ï¼ˆç»å¯¹æ—¶é—´ï¼‰</span>
              <span className={`text-xs px-2 py-1 rounded-full ${
                isRecording 
                  ? 'bg-red-500/10 text-red-600 border border-red-500/20' 
                  : 'bg-muted text-muted-foreground'
              }`}>
                {isRecording ? 'â— å½•éŸ³ä¸­' : 'â—‹ ç©ºé—²'}
              </span>
            </div>
            <div className="flex-1 min-h-0">
              <WaveformTimeline 
                analyser={analyser}
                isRecording={isRecording}
                timeline={timeline}
                audioSegments={audioSegments}
                schedules={schedules}
                onSeek={handleSeek}
                onTimelineChange={handleTimelineChange}
                onZoomChange={setTimelineZoom}
              />
            </div>
          </div>
          <div className="flex-1 flex flex-col min-h-0 bg-background">
            <div className="px-4 py-2 text-xs font-semibold text-muted-foreground uppercase tracking-wider flex justify-between items-center bg-card/50 border-b border-border">
              <span>è½¬å½•æ–‡æœ¬ï¼ˆç‚¹å‡»æ–‡æœ¬å›æ”¾ï¼‰</span>
            </div>
            <TranscriptionLog 
              segments={transcripts} 
              onSegmentClick={handleSegmentClick}
              isRecording={isRecording}
            />
          </div>
        </div>
        <div className="md:col-span-1 h-full min-h-0 overflow-hidden flex flex-col border-l border-border">
          <div className="h-48 p-4 border-b border-border bg-card/50 overflow-y-auto">
            <h3 className="text-sm font-semibold text-foreground mb-3">æ—¥ç¨‹åˆ—è¡¨</h3>
            <ScheduleList schedules={schedules} />
          </div>
          <div className="flex-1 min-h-0">
            <ChatInterface 
              messages={chatMessages} 
              onSendMessage={handleSendMessage} 
              isLoading={isChatLoading} 
            />
          </div>
        </div>
      </main>
    </div>
  );
}

